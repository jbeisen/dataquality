{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0060a3da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üî≠ Logging you into Galileo\n",
      "\n",
      "üîê How would you like to login? \n",
      "Enter one of the following: email\n",
      "email\n",
      "üöÄ You're logged in to Galileo as anthony@rungalileo.io!\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Part 0. Log in to Galileo!\n",
    "\"\"\"\n",
    "\n",
    "import dataquality\n",
    "\n",
    "dataquality.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "50131e3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ú® Initializing project remote_plum_eagle\n",
      "üèÉ‚Äç‚ôÇÔ∏è Starting run permanent_teal_hookworm\n",
      "üõ∞ Created project, remote_plum_eagle, and new run, permanent_teal_hookworm.\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Part 0.1 Create your first project!\n",
    "\"\"\"\n",
    "\n",
    "dataquality.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e55cb3ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Config(api_url='http://localhost:8000', auth_method=<AuthMethod.email: 'email'>, token='eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9.eyJzdWIiOiJhbnRob255QHJ1bmdhbGlsZW8uaW8iLCJleHAiOjE2MzI1MTE2ODF9.WpA2FzJwPoXL2nN-911pLu0qwTOZh4DzjFw1l6EfIKs', current_user='anthony@rungalileo.io', current_project_id='7913fab2-7269-400a-8cc8-efb1b2b57120', current_run_id='5154fd19-6432-4074-bcab-a2f0c4406116')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataquality.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6854d67e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in /Users/anthcor/dataquality/.venv/lib/python3.9/site-packages (1.9.0)\n",
      "Requirement already satisfied: sklearn in /Users/anthcor/dataquality/.venv/lib/python3.9/site-packages (0.0)\n",
      "Requirement already satisfied: transformers in /Users/anthcor/dataquality/.venv/lib/python3.9/site-packages (4.9.2)\n",
      "Requirement already satisfied: pandas in /Users/anthcor/dataquality/.venv/lib/python3.9/site-packages (1.3.2)\n",
      "Requirement already satisfied: numpy in /Users/anthcor/dataquality/.venv/lib/python3.9/site-packages (1.21.2)\n",
      "Requirement already satisfied: pytorch_lightning in /Users/anthcor/dataquality/.venv/lib/python3.9/site-packages (1.4.4)\n",
      "Requirement already satisfied: torchmetrics in /Users/anthcor/dataquality/.venv/lib/python3.9/site-packages (0.5.0)\n",
      "Requirement already satisfied: typing-extensions in /Users/anthcor/dataquality/.venv/lib/python3.9/site-packages (from torch) (3.10.0.0)\n",
      "Requirement already satisfied: scikit-learn in /Users/anthcor/dataquality/.venv/lib/python3.9/site-packages (from sklearn) (0.24.2)\n",
      "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /Users/anthcor/dataquality/.venv/lib/python3.9/site-packages (from transformers) (0.10.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /Users/anthcor/dataquality/.venv/lib/python3.9/site-packages (from transformers) (4.62.2)\n",
      "Requirement already satisfied: filelock in /Users/anthcor/dataquality/.venv/lib/python3.9/site-packages (from transformers) (3.0.12)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/anthcor/dataquality/.venv/lib/python3.9/site-packages (from transformers) (5.4.1)\n",
      "Requirement already satisfied: huggingface-hub==0.0.12 in /Users/anthcor/dataquality/.venv/lib/python3.9/site-packages (from transformers) (0.0.12)\n",
      "Requirement already satisfied: requests in /Users/anthcor/dataquality/.venv/lib/python3.9/site-packages (from transformers) (2.26.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Users/anthcor/dataquality/.venv/lib/python3.9/site-packages (from transformers) (2021.8.3)\n",
      "Requirement already satisfied: packaging in /Users/anthcor/dataquality/.venv/lib/python3.9/site-packages (from transformers) (21.0)\n",
      "Requirement already satisfied: sacremoses in /Users/anthcor/dataquality/.venv/lib/python3.9/site-packages (from transformers) (0.0.45)\n",
      "Requirement already satisfied: pytz>=2017.3 in /Users/anthcor/dataquality/.venv/lib/python3.9/site-packages (from pandas) (2021.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /Users/anthcor/dataquality/.venv/lib/python3.9/site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: fsspec[http]!=2021.06.0,>=2021.05.0 in /Users/anthcor/dataquality/.venv/lib/python3.9/site-packages (from pytorch_lightning) (2021.7.0)\n",
      "Requirement already satisfied: future>=0.17.1 in /Users/anthcor/dataquality/.venv/lib/python3.9/site-packages (from pytorch_lightning) (0.18.2)\n",
      "Requirement already satisfied: pyDeprecate==0.3.1 in /Users/anthcor/dataquality/.venv/lib/python3.9/site-packages (from pytorch_lightning) (0.3.1)\n",
      "Requirement already satisfied: tensorboard>=2.2.0 in /Users/anthcor/dataquality/.venv/lib/python3.9/site-packages (from pytorch_lightning) (2.6.0)\n",
      "Requirement already satisfied: aiohttp in /Users/anthcor/dataquality/.venv/lib/python3.9/site-packages (from fsspec[http]!=2021.06.0,>=2021.05.0->pytorch_lightning) (3.7.4.post0)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /Users/anthcor/dataquality/.venv/lib/python3.9/site-packages (from packaging->transformers) (2.4.7)\n",
      "Requirement already satisfied: six>=1.5 in /Users/anthcor/dataquality/.venv/lib/python3.9/site-packages (from python-dateutil>=2.7.3->pandas) (1.16.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /Users/anthcor/dataquality/.venv/lib/python3.9/site-packages (from tensorboard>=2.2.0->pytorch_lightning) (3.3.4)\n",
      "Requirement already satisfied: wheel>=0.26 in /Users/anthcor/dataquality/.venv/lib/python3.9/site-packages (from tensorboard>=2.2.0->pytorch_lightning) (0.37.0)\n",
      "Requirement already satisfied: grpcio>=1.24.3 in /Users/anthcor/dataquality/.venv/lib/python3.9/site-packages (from tensorboard>=2.2.0->pytorch_lightning) (1.39.0)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /Users/anthcor/dataquality/.venv/lib/python3.9/site-packages (from tensorboard>=2.2.0->pytorch_lightning) (1.8.0)\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /Users/anthcor/dataquality/.venv/lib/python3.9/site-packages (from tensorboard>=2.2.0->pytorch_lightning) (0.6.1)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in /Users/anthcor/dataquality/.venv/lib/python3.9/site-packages (from tensorboard>=2.2.0->pytorch_lightning) (2.0.1)\n",
      "Requirement already satisfied: google-auth<2,>=1.6.3 in /Users/anthcor/dataquality/.venv/lib/python3.9/site-packages (from tensorboard>=2.2.0->pytorch_lightning) (1.35.0)\n",
      "Requirement already satisfied: absl-py>=0.4 in /Users/anthcor/dataquality/.venv/lib/python3.9/site-packages (from tensorboard>=2.2.0->pytorch_lightning) (0.13.0)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /Users/anthcor/dataquality/.venv/lib/python3.9/site-packages (from tensorboard>=2.2.0->pytorch_lightning) (0.4.5)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /Users/anthcor/dataquality/.venv/lib/python3.9/site-packages (from tensorboard>=2.2.0->pytorch_lightning) (56.0.0)\n",
      "Requirement already satisfied: protobuf>=3.6.0 in /Users/anthcor/dataquality/.venv/lib/python3.9/site-packages (from tensorboard>=2.2.0->pytorch_lightning) (3.17.3)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /Users/anthcor/dataquality/.venv/lib/python3.9/site-packages (from google-auth<2,>=1.6.3->tensorboard>=2.2.0->pytorch_lightning) (0.2.8)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /Users/anthcor/dataquality/.venv/lib/python3.9/site-packages (from google-auth<2,>=1.6.3->tensorboard>=2.2.0->pytorch_lightning) (4.7.2)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /Users/anthcor/dataquality/.venv/lib/python3.9/site-packages (from google-auth<2,>=1.6.3->tensorboard>=2.2.0->pytorch_lightning) (4.2.2)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /Users/anthcor/dataquality/.venv/lib/python3.9/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.2.0->pytorch_lightning) (1.3.0)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /Users/anthcor/dataquality/.venv/lib/python3.9/site-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard>=2.2.0->pytorch_lightning) (0.4.8)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/anthcor/dataquality/.venv/lib/python3.9/site-packages (from requests->transformers) (3.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/anthcor/dataquality/.venv/lib/python3.9/site-packages (from requests->transformers) (2021.5.30)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /Users/anthcor/dataquality/.venv/lib/python3.9/site-packages (from requests->transformers) (2.0.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/anthcor/dataquality/.venv/lib/python3.9/site-packages (from requests->transformers) (1.26.6)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /Users/anthcor/dataquality/.venv/lib/python3.9/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.2.0->pytorch_lightning) (3.1.1)\n",
      "Requirement already satisfied: async-timeout<4.0,>=3.0 in /Users/anthcor/dataquality/.venv/lib/python3.9/site-packages (from aiohttp->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch_lightning) (3.0.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /Users/anthcor/dataquality/.venv/lib/python3.9/site-packages (from aiohttp->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch_lightning) (5.1.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /Users/anthcor/dataquality/.venv/lib/python3.9/site-packages (from aiohttp->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch_lightning) (1.6.3)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /Users/anthcor/dataquality/.venv/lib/python3.9/site-packages (from aiohttp->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch_lightning) (21.2.0)\n",
      "Requirement already satisfied: chardet<5.0,>=2.0 in /Users/anthcor/dataquality/.venv/lib/python3.9/site-packages (from aiohttp->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch_lightning) (4.0.0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: joblib in /Users/anthcor/dataquality/.venv/lib/python3.9/site-packages (from sacremoses->transformers) (1.0.1)\n",
      "Requirement already satisfied: click in /Users/anthcor/dataquality/.venv/lib/python3.9/site-packages (from sacremoses->transformers) (8.0.1)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /Users/anthcor/dataquality/.venv/lib/python3.9/site-packages (from scikit-learn->sklearn) (2.2.0)\n",
      "Requirement already satisfied: scipy>=0.19.1 in /Users/anthcor/dataquality/.venv/lib/python3.9/site-packages (from scikit-learn->sklearn) (1.7.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Part 0.2 Install some dependencies for this workflow exercise.\n",
    "\"\"\"\n",
    "\n",
    "%pip install torch sklearn transformers pandas numpy pytorch_lightning torchmetrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9c1487f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Part 1.\n",
    "\n",
    "Log your datasets with Galileo.\n",
    "\n",
    "Create the Newsgroup dataset class. Using huggingface Bert Tokenizer.\n",
    "\n",
    "We are introducing some noise to these datasets because \n",
    "the newsgroup dataset is already well labeled.\n",
    "\"\"\"\n",
    "\n",
    "import torch\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from transformers import DistilBertTokenizerFast\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def introduce_label_errors(df: pd.DataFrame, column: str, shuffle_percent: int) -> pd.DataFrame:\n",
    "    arr = df[column].values\n",
    "    shuffle = np.random.choice(\n",
    "        np.arange(arr.shape[0]), \n",
    "        round(arr.shape[0] * shuffle_percent / 100), \n",
    "        replace=False)\n",
    "    arr[np.sort(shuffle)] = arr[shuffle]\n",
    "    df[column] = arr\n",
    "    return df\n",
    "    \n",
    "\n",
    "class NewsgroupDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, split: str) -> None:\n",
    "        newsgroups = fetch_20newsgroups(subset=split, \n",
    "                                        remove=('headers', 'footers', 'quotes'))\n",
    "\n",
    "        self.dataset = pd.DataFrame()\n",
    "        self.dataset[\"text\"] = newsgroups.data\n",
    "        self.dataset[\"label\"] = newsgroups.target\n",
    "        self.dataset = self.dataset[:100]\n",
    "\n",
    "        # Shuffle some percentage of the training dataset \n",
    "        # to force create mislabeled samples\n",
    "        if split == \"train\":\n",
    "            self.dataset = introduce_label_errors(self.dataset, \"label\", 11)\n",
    "\n",
    "        #\n",
    "        # üî≠ Logging Inputs with Galileo!\n",
    "        #\n",
    "        for i in range(len(self.dataset)):\n",
    "            dataquality.log_input_data({\n",
    "                \"id\": i,\n",
    "                \"text\": self.dataset[\"text\"][i],\n",
    "                \"gold\": str(self.dataset[\"label\"][i]),\n",
    "                \"split\": \"training\" if split == \"train\" else \"test\"})\n",
    "\n",
    "        tokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased')\n",
    "        self.encodings = tokenizer(self.dataset[\"text\"].tolist(), truncation=True, padding=True)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        x = torch.tensor(self.encodings[\"input_ids\"][idx])\n",
    "        attention_mask = torch.tensor(self.encodings[\"attention_mask\"][idx])\n",
    "        y = self.dataset[\"label\"][idx]\n",
    "        return idx, x, attention_mask, y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a9ed9fa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Part 2.\n",
    "\n",
    "Log model outputs with Galileo.\n",
    "\n",
    "We are using a DistilBERT pytorch lightning class for text classification.\n",
    "\"\"\"\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "from transformers import DistilBertForSequenceClassification, AdamW, DistilBertConfig\n",
    "import torch.nn.functional as F\n",
    "import torchmetrics\n",
    "\n",
    "class LightningDistilBERT(pl.LightningModule):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.model = DistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased', config=DistilBertConfig(num_labels=20))\n",
    "        self.train_acc = torchmetrics.Accuracy()\n",
    "        self.val_acc = torchmetrics.Accuracy()\n",
    "        self.test_acc = torchmetrics.Accuracy()\n",
    "\n",
    "    def forward(self, x, attention_mask, x_idxs, epoch, split):\n",
    "        out = self.model(x, attention_mask=attention_mask)\n",
    "        log_probs = F.log_softmax(out.logits, dim=1)\n",
    "        probs = F.softmax(out.logits, dim=1)\n",
    "        if x_idxs is not None:\n",
    "            for i in range(len(x_idxs)):\n",
    "                index = int(x_idxs[i])\n",
    "                prob = probs[i].detach().cpu().numpy().tolist()\n",
    "                #\n",
    "                # üî≠ Logging outputs with Galileo!\n",
    "                #\n",
    "                dataquality.log_model_output({\n",
    "                    \"id\": int(x_idxs[i]),\n",
    "                    \"epoch\": epoch,\n",
    "                    \"split\": \"training\" if split == \"train\" else \"test\",\n",
    "                    \"emb\": [0.0],\n",
    "                    \"prob\": probs[i].detach().cpu().numpy().tolist()})\n",
    "        return log_probs\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        \"\"\"Model training step.\"\"\"\n",
    "        x_idxs, x, attention_mask, y = batch\n",
    "        log_probs = self(x=x, attention_mask=attention_mask, x_idxs=x_idxs, epoch=self.current_epoch, split=\"training\")\n",
    "        loss = F.nll_loss(log_probs, y)\n",
    "        self.train_acc(torch.argmax(log_probs, 1), y)\n",
    "        self.log(\"train_acc\", self.train_acc, prog_bar=True)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        \"\"\"Model validation step.\"\"\"\n",
    "        x_idxs, x, attention_mask, y = batch\n",
    "        log_probs = self(x=x, attention_mask=attention_mask, x_idxs=x_idxs, epoch=self.current_epoch, split=\"validation\")\n",
    "        loss = F.nll_loss(log_probs, y)\n",
    "        self.val_acc(torch.argmax(log_probs, 1), y)\n",
    "        self.log(\"val_acc\", self.val_acc, prog_bar=True)\n",
    "        return loss\n",
    "\n",
    "    def test_step(self, batch, batch_idx): \n",
    "        \"\"\"Model test step.\"\"\"\n",
    "        x_idxs, x, attention_mask, y = batch\n",
    "        log_probs = self(x=x, attention_mask=attention_mask, x_idxs=x_idxs, epoch=self.current_epoch, split=\"test\")\n",
    "        loss = F.nll_loss(log_probs, y)\n",
    "        self.test_acc(torch.argmax(log_probs, 1), y)\n",
    "        self.log(\"test_acc\", self.test_acc, prog_bar=True)\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        \"\"\"Model optimizers.\"\"\"\n",
    "        return torch.optim.AdamW(filter(lambda p: p.requires_grad, self.parameters()), lr=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9bdd6325",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_transform.bias', 'vocab_layer_norm.weight', 'vocab_transform.weight', 'vocab_layer_norm.bias', 'vocab_projector.weight', 'vocab_projector.bias']\n",
      "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.bias', 'pre_classifier.weight', 'classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "\n",
      "  | Name      | Type                                | Params\n",
      "------------------------------------------------------------------\n",
      "0 | model     | DistilBertForSequenceClassification | 67.0 M\n",
      "1 | train_acc | Accuracy                            | 0     \n",
      "2 | val_acc   | Accuracy                            | 0     \n",
      "3 | test_acc  | Accuracy                            | 0     \n",
      "------------------------------------------------------------------\n",
      "67.0 M    Trainable params\n",
      "0         Non-trainable params\n",
      "67.0 M    Total params\n",
      "267.875   Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation sanity check: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/anthcor/dataquality/.venv/lib/python3.9/site-packages/pytorch_lightning/trainer/data_loading.py:372: UserWarning: Your val_dataloader has `shuffle=True`, it is best practice to turn this off for val/test/predict dataloaders.\n",
      "  rank_zero_warn(\n",
      "/Users/anthcor/dataquality/.venv/lib/python3.9/site-packages/pytorch_lightning/trainer/data_loading.py:105: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/Users/anthcor/dataquality/.venv/lib/python3.9/site-packages/pytorch_lightning/trainer/data_loading.py:105: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/Users/anthcor/dataquality/.venv/lib/python3.9/site-packages/pytorch_lightning/trainer/data_loading.py:322: UserWarning: The number of training samples (13) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4cda1add880d4c5398c2909922fee222",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: -1it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/anthcor/dataquality/.venv/lib/python3.9/site-packages/pytorch_lightning/trainer/data_loading.py:372: UserWarning: Your test_dataloader has `shuffle=True`, it is best practice to turn this off for val/test/predict dataloaders.\n",
      "  rank_zero_warn(\n",
      "/Users/anthcor/dataquality/.venv/lib/python3.9/site-packages/pytorch_lightning/trainer/data_loading.py:105: UserWarning: The dataloader, test dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2900a18cec044b03af7268d404146eef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "DATALOADER:0 TEST RESULTS\n",
      "{'test_acc': 0.03999999910593033}\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'test_acc': 0.03999999910593033}]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Part 3.\n",
    "\n",
    "Instantiate a model and train it with PyTorch Lightning.\n",
    "\"\"\"\n",
    "\n",
    "model = LightningDistilBERT()\n",
    "train_dataloader = torch.utils.data.DataLoader(NewsgroupDataset(\"train\"), batch_size=8, shuffle=True)\n",
    "test_dataloader = torch.utils.data.DataLoader(NewsgroupDataset(\"test\"), batch_size=8, shuffle=True)\n",
    "\n",
    "trainer = pl.Trainer(max_epochs=2)\n",
    "\n",
    "trainer.fit(model, train_dataloader, test_dataloader)\n",
    "trainer.test(model, test_dataloader)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
