{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c3cd9e0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "import torch\n",
    "from transformers import DistilBertTokenizerFast\n",
    "from galileo import Galileo\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class NewsgroupDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, split: str, g: Galileo):\n",
    "\n",
    "        newsgroups = fetch_20newsgroups(subset=split, remove=('headers', 'footers', 'quotes'))\n",
    "\n",
    "        self.dataset = pd.DataFrame()\n",
    "        self.dataset[\"text\"] = newsgroups.data\n",
    "        self.dataset[\"label\"] = newsgroups.target\n",
    "\n",
    "        for i in range(len(self.dataset)):\n",
    "            payload = {\n",
    "                \"id\": i,\n",
    "                \"text\": self.dataset[\"text\"][i],\n",
    "                \"gold\": str(self.dataset[\"label\"][i]),\n",
    "            }\n",
    "            g.logger.log_input(payload,\n",
    "                               logger_mode=\"training\" if split == \"train\" else \"test\")\n",
    "\n",
    "        if split == \"train\":\n",
    "            g.logger.log_labels(newsgroups.target_names)\n",
    "\n",
    "        tokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased')\n",
    "        self.encodings = tokenizer(self.dataset[\"text\"].tolist(), truncation=True, padding=True)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x = torch.tensor(self.encodings[\"input_ids\"][idx])\n",
    "        attention_mask = torch.tensor(self.encodings[\"attention_mask\"][idx])\n",
    "        y = self.dataset[\"label\"][idx]\n",
    "        return idx, x, attention_mask, y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "de412a99",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytorch_lightning as pl\n",
    "from transformers import DistilBertForSequenceClassification, AdamW, DistilBertConfig\n",
    "import torch.nn.functional as F\n",
    "import torchmetrics\n",
    "\n",
    "class LightningDistilBERT(pl.LightningModule):\n",
    "    def __init__(self, g: Galileo):\n",
    "        super().__init__()\n",
    "        self.model = DistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased', config=DistilBertConfig(num_labels=20))\n",
    "\n",
    "        self.g = g\n",
    "\n",
    "        self.train_acc = torchmetrics.Accuracy()\n",
    "        self.val_acc = torchmetrics.Accuracy()\n",
    "        self.test_acc = torchmetrics.Accuracy()\n",
    "    \n",
    "    def forward(self, x, attention_mask, x_idxs=None, epoch=None, logging=False):\n",
    "        out = self.model(x, attention_mask=attention_mask)\n",
    "\n",
    "        log_probs = F.log_softmax(out.logits, dim=1)\n",
    "\n",
    "        probs = F.softmax(out.logits, dim=1)  # This should be exp(x)\n",
    "\n",
    "        if logging and x_idxs is not None and epoch is not None:\n",
    "            for i in range(len(x_idxs)):\n",
    "                index = int(x_idxs[i])\n",
    "                prob = probs[i].detach().cpu().numpy().tolist()\n",
    "                self.g.logger.log_output(\n",
    "                    {\n",
    "                        \"epoch\": epoch,\n",
    "                        \"id\": index,\n",
    "                        \"emb\": [0.0],\n",
    "                        \"prob\": prob,\n",
    "                    }\n",
    "                )\n",
    "        return log_probs\n",
    "    \n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        \"\"\"Model training step.\"\"\"\n",
    "        x_idxs, x, attention_mask, y = batch\n",
    "        log_probs = self(x, attention_mask, x_idxs, self.current_epoch, True)\n",
    "        loss = F.nll_loss(log_probs, y)\n",
    "        self.train_acc(torch.argmax(log_probs, 1), y)\n",
    "        self.log(\"train_acc\", self.train_acc, prog_bar=True)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        \"\"\"Model validation step.\"\"\"\n",
    "        x_idxs, x, attention_mask, y = batch\n",
    "        log_probs = self(x, attention_mask, logging=False)\n",
    "        loss = F.nll_loss(log_probs, y)\n",
    "        self.val_acc(torch.argmax(log_probs, 1), y)\n",
    "        self.log(\"val_acc\", self.val_acc, prog_bar=True)\n",
    "        return loss\n",
    "\n",
    "    def test_step(self, batch, batch_idx):  # Using this to log galileo\n",
    "        \"\"\"Model test step.\"\"\"\n",
    "        x_idxs, x, attention_mask, y = batch\n",
    "        log_probs = self(x, attention_mask, x_idxs, -1, True)\n",
    "        loss = F.nll_loss(log_probs, y)\n",
    "        self.test_acc(torch.argmax(log_probs, 1), y)\n",
    "        self.log(\"test_acc\", self.test_acc, prog_bar=True)\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        \"\"\"Model optimizers.\"\"\"\n",
    "        return torch.optim.AdamW(\n",
    "            filter(lambda p: p.requires_grad, self.parameters()), lr=1e-5\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "88219901",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_lightning.callbacks import Callback\n",
    "\n",
    "\n",
    "class MyCustomCallback(Callback):\n",
    "    def on_init_start(self, trainer):\n",
    "        print(\"#Atin Starting to init trainer!\")\n",
    "\n",
    "    def on_init_end(self, trainer):\n",
    "        print(\"#Atin Trainer is init now\")\n",
    "\n",
    "    def on_train_end(self, trainer, pl_module):\n",
    "        print(\"#Atin Do something when training ends\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7df0037d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_lightning.loggers import WandbLogger\n",
    "from pytorch_lightning.callbacks.early_stopping import EarlyStopping\n",
    "\n",
    "# Initialize Galileo session\n",
    "g = Galileo(namespace=\"newsgroup_demo\")\n",
    "\n",
    "model = LightningDistilBERT(g)\n",
    "train_dataloader = torch.utils.data.DataLoader(NewsgroupDataset(\"train\", g), batch_size=8, shuffle=True)\n",
    "val_dataloader = torch.utils.data.DataLoader(NewsgroupDataset(\"test\", g), batch_size=8, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "417548d1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a107b66f",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = pl.Trainer(\n",
    "    gpus=-1 if torch.cuda.is_available() else None,\n",
    "    max_epochs=2,\n",
    "    callbacks=[MyCustomCallback()]\n",
    ")\n",
    "\n",
    "trainer.fit(model, train_dataloader, val_dataloader)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
